\chapter{History of Large Language Models}

%==============================================================================
%
%==============================================================================

\section{The History of Large Language Models}

\subsection{The Beginnings: From Vision to the First Machine Translation}

The idea of machines that can understand and use language dates back a long way.
As early as the 1950s, Alan Turing laid the groundwork for computational linguistics
with his vision of “thinking machines”. He developed the famous Turing Test to
determine whether a machine could communicate so convincingly that it was
indistinguishable from a human. This concept inspired many early chatbot systems.

A practical example of machine language processing was the Georgetown-IBM
experiment in 1954, which could automatically translate simple sentences.
It soon became clear that language is not just words and grammar, but also context,
meaning, and nuance—a major challenge for machines.

\subsection{The 1970s and 1980s: Rule-Based Systems and Symbolic AI}

In the following decades, researchers relied on rule-based systems that analyzed
language using fixed patterns. ELIZA (1966) was one of the most well-known early
programs of this type. It simulated therapeutic conversations by recognizing keywords
and returning pre-defined responses—without any real language understanding.

Another milestone was SHRDLU (1970), a system that interpreted simple verbal commands
in a simulated block world. These early efforts showed that while AI could process
language, it was still heavily dependent on manually crafted rules and responses.

\subsection{The 1990s: Statistics Over Rules --- The Rise of Probabilistic Models}

With the increasing availability of large text corpora, researchers began to use
statistical methods instead of fixed rules. N-gram models analyzed word sequence
probabilities, producing text that appeared more natural.

IBM’s work in statistical machine translation was especially groundbreaking.
These systems performed much better than earlier rule-based approaches and laid the
foundation for modern translation technologies. However, they computed only
probabilities, without a deeper understanding of language.

\subsection{The 2000s: The Rise of Neural Networks and Deep Learning}

Advances in artificial neural networks marked a major breakthrough in the 2000s.
Recurrent Neural Networks (RNNs) and especially Long Short-Term Memory (LSTM)
networks greatly improved the processing of language sequences.

A revolutionary step was the introduction of word embeddings. While older systems
treated language as a mere sequence of words, models like word2vec (2013) mapped words
into a multidimensional space, making it possible to compute semantic similarities
(e.g., “king” and “queen” are related, or “Paris” belongs to “France”).

\subsection{The 2010s: Transformers --- The Revolution in Language Processing}

In 2017 the breakthrough came with the Transformer architecture by Vaswani et al.
This model employed self-attention to analyze a word’s context across the entire
sentence rather than only its neighbors. This approach quickly became the standard
for nearly all modern language models.

In 2018, Google introduced BERT, a model that processed text bidirectionally.
BERT revolutionized many NLP tasks by understanding words in the context of their
surroundings.

\subsection{The 2020s: Generative AI and the Breakthrough of Large Language Models}

In 2020, OpenAI’s GPT-3 made headlines. With 175 billion parameters, it was the most
powerful language model of its time, capable of generating fluent text, writing code,
and answering questions impressively.

Then, with ChatGPT (2022), AI became truly accessible. Suddenly, anyone could chat
with an AI that responded in natural language, explained complex topics, and even
wrote creative texts. The introduction of GPT-4 (2023) and other multimodal models,
which can process text alongside images and other data, expanded AI’s versatility.

\subsection{Current Trends: The Future of Language AI}

Today, in 2025, AI is evolving rapidly:
\begin{itemize}
  \item Multimodal models can analyze not only text but also images, videos, and audio.
  \item AI agents are taking on complex tasks autonomously and interacting with other
        systems.
  \item Ethical challenges are coming into focus to prevent misuse, biases, and
        misinformation.
\end{itemize}

\subsubsection{Key Factors Driving Progress}

Two essential factors have made the development of language models possible:
\begin{enumerate}
  \item \textbf{Large Data Sets:} AI models require enormous text corpora to learn
        language effectively.
  \item \textbf{Computing Power:} Advances in hardware, especially high-performance GPUs
        and TPUs, have enabled the training of ever larger models.
\end{enumerate}

\subsubsection{Ethical and Societal Considerations}

As AI grows more powerful, new challenges arise:
\begin{itemize}
  \item \textbf{Risks:} The spread of misinformation, biases in models, and potential
        misuse by fraudsters or manipulators.
  \item \textbf{Opportunities:} Enhanced communication, task automation, and entirely
        new possibilities for research, education, and creativity.
\end{itemize}

The impact of these developments on our worldview and understanding of humanity will
be explored further in this book.

%------------------------------------------------------------------------------
% The Georgetown-IBM Experiment of 1954
%------------------------------------------------------------------------------

\section{The Georgetown-IBM Experiment of 1954}

The Georgetown-IBM Experiment of 1954 marked a significant milestone in the history of
machine translation. On January 7, 1954, scientists from Georgetown University in
collaboration with IBM demonstrated a system that could automatically translate Russian
sentences into English.

\subsection{Background and Objectives}

The main goal of the experiment was to showcase the potential of machine translation and
to attract public and governmental support for further research. Although the system was
limited in scope, it impressively demonstrated the capabilities of computer technology
in language processing.

\subsection{Technical Details}

The translation system was based on the IBM 701, one of IBM’s first commercial scientific
computers. It had a vocabulary of about 250 words and six grammar rules. The vocabulary
covered fields such as politics, law, mathematics, chemistry, metallurgy, communications,
and military affairs. Words were stored on punch cards and processed by the computer.
The translation was fully automatic, with no human intervention during the process.

\subsection{Experiment Process}

During the demonstration, more than 60 Russian sentences (transliterated into Latin)
were entered into the computer by an operator who did not understand Russian. The IBM
701 produced the corresponding English translations within seconds. The selected
sentences covered topics from politics and law to mathematics and natural sciences.

\subsection{Examples of Translated Sentences}

Some examples of the translated sentences are:
\begin{itemize}
  \item \textbf{Russian (transliterated):} Mi pyeryedayem mislyi posryedstvom ryechyi.\\
        \textbf{English:} We transmit thoughts by means of speech.
  \item \textbf{Russian (transliterated):} Vyelyichyina ugla opryedyelyayetsya otnoshyenyiyem 
        dlyini dugi k radyiusu.\\
        \textbf{English:} The magnitude of an angle is determined by the ratio of the arc
        length to the radius.
  \item \textbf{Russian (transliterated):} Myezhdunarodnoye ponyimanyiye yavlyayetsya vazhnim 
        faktorom v ryeshyenyiyi polyityichyeskix voprosov.\\
        \textbf{English:} International understanding is an important factor in resolving
        political issues.
\end{itemize}

\subsection{Reception and Impact}

The demonstration received widespread media attention and was hailed as a great success.
It raised high expectations for automatic translation systems and led to increased
investments in research. Although the developers were optimistic that machine translation
could be solved within three to five years, the actual progress turned out to be far more
complex and time-consuming.

Overall, the Georgetown-IBM Experiment laid the groundwork for future research and
development in machine translation and greatly influenced the field of computational
linguistics.

%------------------------------------------------------------------------------
% ELIZA --- The First Chatbot in History
%------------------------------------------------------------------------------

\section{ELIZA --- The First Chatbot in History}

ELIZA is one of the most well-known early programs for natural language processing and
is often regarded as the first chatbot in history. Developed between 1964 and 1966 by
Joseph Weizenbaum at MIT, ELIZA was designed to show that machines could simulate
human-like interactions using simple linguistic tricks.

\subsection{How ELIZA Worked}

ELIZA used a pattern-based approach, recognizing keywords in user inputs and returning
pre-formulated responses by repeating or rephrasing parts of the input. It did not
possess true language understanding but operated using simple rules and scripted patterns.

Its best-known script, the “DOCTOR” script, simulated a psychotherapist using a
Rogerian approach, for example:
\begin{itemize}
  \item \textbf{User:} I have problems with my mother.\\
        \textbf{ELIZA:} Tell me more about your mother.
\end{itemize}
This led many users to believe that ELIZA truly “understood” them, even though it was
simply performing rule-based text transformations.

\subsection{The Significance of ELIZA}

Although technically simple, ELIZA was the first program to show that people tend to
attribute human characteristics to machine interactions—a tendency Weizenbaum warned
against. He argued that humans should not overly personalize machines since they lack
true understanding.

ELIZA had a lasting impact on the development of artificial intelligence and language
processing, inspiring later chatbots such as PARRY (1972), ALICE (1995), and ultimately
modern systems like ChatGPT.

\subsection{Limitations and Constraints}

ELIZA had several limitations:
\begin{itemize}
  \item It could only manage very limited conversations.
  \item It did not understand context or meaning.
  \item Its responses were generated solely through simple text patterns without any
        actual analysis.
\end{itemize}

\subsection{Publicly Available Literature on ELIZA}

For further information on ELIZA, see:
\begin{enumerate}
  \item The Wikipedia article on ELIZA for an overview of its history, functionality, and
        significance.
  \item Joseph Weizenbaum’s original 1966 paper on ELIZA for a detailed technical description.
  \item The original publication “ELIZA --- A Computer Program for the Study of Natural
        Language Communication between Man and Machine” (1966).
  \item Articles and historical overviews on the evolution from ELIZA to modern chatbots.
  \item Additional background available on MIT’s website regarding early AI research.
\end{enumerate}

%------------------------------------------------------------------------------
% Probabilistic Models in Language Processing in the 1990s
%------------------------------------------------------------------------------

\section{Probabilistic Models in Language Processing in the 1990s}

\subsection{The Paradigm Shift: From Rules to Probabilities}

Until the 1980s, rule-based methods dominated machine language processing. Systems like
ELIZA (1966) and SHRDLU (1970) used pre-defined rules and patterns, making them inflexible
to new expressions and requiring extensive manual adjustments.

In the 1990s, a fundamental shift occurred: statistical probabilistic models were seen as a
more powerful alternative. Instead of manually specifying rules, these models analyzed
large text datasets to identify word and sentence patterns based on probabilities.

\subsection{N-Gram Models: The Foundation of Modern Language Processing}

One of the key developments of this era was the introduction of N-gram models.
An N-gram is a sequence of N consecutive words:
\begin{itemize}
  \item A unigram considers a single word.
  \item A bigram analyzes word pairs (e.g., “good morning”).
  \item A trigram considers three consecutive words (e.g., “I am going today”).
\end{itemize}
These models estimated the probability of a word based on the preceding words. For example,
a bigram model might calculate that “morning” is highly likely to follow “good.”

\subsection{The N-Gram Formula}

In a bigram model, the probability of a sentence is given by:
\[
P(W_1, W_2, \dots, W_n) = P(W_1)\,P(W_2\mid W_1)\,P(W_3\mid W_2)\,
\dots\,P(W_n\mid W_{n-1})
\]
Here, \(P(W_n\mid W_{n-1})\) is the probability of a word given the previous word.

This simple method allowed for the calculation of sentence probabilities and
significantly improved machine translation, speech recognition, and autocomplete features.

\subsection{Statistical Machine Translation (IBM Models)}

IBM developed a series of models in the 1990s that used statistical methods for
translating texts:
\begin{itemize}
  \item Parallel texts (e.g., English-French) were analyzed.
  \item Probabilities were computed to determine which word in one language corresponds
        to a specific word in another.
  \item The more frequently a word pair occurred in the training data, the more likely it was
        to be used in future translations.
\end{itemize}
IBM Models 1--5 (1990–1993) introduced alignment methods to calculate word-to-word
correspondences. Models 4 and 5 further improved these ideas by considering word order.

\subsection{Hidden Markov Models (HMMs) for Speech Recognition}

While N-gram models were used for text, automatic speech recognition systems increasingly
relied on Hidden Markov Models (HMMs):
\begin{itemize}
  \item HMMs are probabilistic models that represent a sequence of states (e.g., spoken sounds).
  \item They were employed in systems such as Dragon Dictate and early versions of Google Voice.
  \item HMMs improved speech recognition by considering both the probability of individual
        words and their phonetic variations.
\end{itemize}

\subsection{Influence and Limitations}

\textbf{Advantages:}
\begin{itemize}
  \item Automated learning from large datasets without explicit rules.
  \item High scalability and flexibility across various languages.
  \item Applications in machine translation, speech recognition, and spell checking.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item Limited context: N-gram models consider only a fixed number of words.
  \item High data requirements: Large text corpora are needed.
  \item Lack of semantic understanding: These models compute probabilities without
        capturing meaning.
\end{itemize}

Despite these drawbacks, probabilistic models were a crucial step towards modern language AI,
paving the way for neural networks and deep learning.

%------------------------------------------------------------------------------
% The Rise of Neural Networks from 2000
%------------------------------------------------------------------------------

\section{The Rise of Neural Networks from 2000}

\subsection{From Statistics to Deep Learning: A Turning Point in AI}

Up to the 1990s, statistical models dominated language processing. By the 2000s, it became
clear that these methods were limited—they struggled with complex semantics and required
massive datasets.

During this period, neural networks experienced a resurgence. Although the concept of
artificial neurons dates back to the 1950s, technological advances in the 2000s made
large-scale neural networks practical.

\subsection{The Renaissance of Neural Networks}

\begin{enumerate}
  \item \textbf{Hardware Advances:} Powerful GPUs made training large networks feasible.
  \item \textbf{Robust Architectures:}
    \begin{itemize}
      \item Multi-Layer Perceptrons (MLPs) were initially too shallow to capture deep
            patterns.
      \item Recurrent Neural Networks (RNNs), especially LSTMs (introduced in 1997),
            improved sequence learning for language.
    \end{itemize}
  \item \textbf{New Training Methods:}
    \begin{itemize}
      \item Backpropagation with improved optimization algorithms (e.g., Adam from 2014)
            enabled faster, more stable learning.
      \item Dropout techniques (introduced in 2012) helped reduce overfitting.
    \end{itemize}
\end{enumerate}

\subsection{Deep Learning Takes Over}

\begin{itemize}
  \item \textbf{2006:} Geoffrey Hinton and colleagues introduced Deep Belief Networks,
        showing that deep architectures could outperform traditional methods.
  \item \textbf{2010s:} Deep learning surpassed classical statistical models in almost every
        AI domain, from image recognition to language processing.
\end{itemize}

\subsection{Breakthroughs in Speech Recognition and Machine Translation}

\begin{itemize}
  \item \textbf{2011:} Apple introduced Siri, one of the first voice assistants using neural
        network recognition.
  \item \textbf{2012:} Google Voice began using neural networks, achieving drastic improvements
        in speech recognition.
  \item \textbf{2016:} AlphaGo defeated the world champion in Go, a major milestone based on deep
        neural networks.
  \item \textbf{2016:} Google Translate shifted from statistical to neural machine translation,
        dramatically improving translation quality.
\end{itemize}

\subsection{Challenges and Limitations}

Neural networks also faced significant challenges:
\begin{itemize}
  \item High computational costs for training large models.
  \item Dependence on vast amounts of training data.
  \item Limited interpretability: Neural networks often act as “black boxes” with
        unclear decision processes.
\end{itemize}

These challenges spurred the development of new models, such as Transformers, which
set a new standard for language AI from 2017 onward.

%------------------------------------------------------------------------------
% The Vector Representation of Language and Its Significance
%------------------------------------------------------------------------------

\section{The Vector Representation of Language and Its Significance}

\subsection{From Text to Vectors: A Revolution in Language Processing}

Traditionally, language in AI systems was treated as a mere sequence of characters or words.
Simple statistical models and early neural networks could not capture complex semantic
relationships.

The breakthrough came with the vector representation of language. Words, sentences, or even
entire documents are now represented as mathematical vectors in a multidimensional space,
fundamentally changing how machines understand and store language.

\subsection{Word Embeddings: The Foundation}

The first major progress in vector representation came with word embeddings:
\begin{itemize}
  \item \textbf{Word2Vec (2013, Google):} The first widely used model to convert words into
        dense vectors, enabling calculations such as:
        \[
        \mytext{"king"} - \mytext{"man"} + \mytext{"woman"} \approx \mytext{"queen"}
        \]
  \item \textbf{GloVe (2014, Stanford):} An alternative based on word co-occurrence statistics,
        yielding accurate vector representations for semantic similarity.
\end{itemize}

\subsection{Representing Sentences and Documents}

While word embeddings capture individual words, later models represented whole sentences
or paragraphs as vectors. Models such as BERT (2018) or Sentence Transformers emerged:
\begin{itemize}
  \item \textbf{BERT \& Transformers:} Provide contextual word vectors that account for a word's
        meaning within a sentence.
  \item \textbf{Sentence-BERT (SBERT, 2019):} Converts entire sentences into vectors for faster,
        more accurate semantic search.
\end{itemize}

\subsection{Vector Databases: A New Infrastructure}

The advent of word and sentence embeddings created the need for efficient databases to store
and search high-dimensional vectors:
\begin{itemize}
  \item \textbf{Definition:} Vector databases store data in specialized index structures rather
        than traditional tables.
  \item \textbf{Notable Examples:}
    \begin{itemize}
      \item FAISS (Facebook AI Similarity Search)
      \item Pinecone
      \item Weaviate
      \item Milvus
    \end{itemize}
\end{itemize}

\subsection{Applications and Importance}

Vector representations enable:
\begin{itemize}
  \item \textbf{Semantic Search:} Comparing meanings instead of exact keywords.
  \item \textbf{Recommendation Systems:} Used by platforms like Netflix and Spotify.
  \item \textbf{Chatbots and AI Assistants:} Faster retrieval of responses, as seen in ChatGPT.
  \item \textbf{Plagiarism Detection:} Automated text comparison in academia and publishing.
\end{itemize}

The combination of neural networks and vector databases now forms the backbone of modern
AI language processing.

%------------------------------------------------------------------------------
% The Transformer Revolution (2010--2020): The Rise of Modern Language Models
%------------------------------------------------------------------------------

\section{The Transformer Revolution (2010--2020): The Rise of Modern Language Models}

\subsection{A Decade of Transformation}

Between 2010 and 2020, AI-driven language processing underwent a radical transformation.
Early models such as RNNs and LSTMs had their limitations, and the breakthrough came in 2017
with the Transformer architecture, which laid the foundation for modern large language models.

\subsection{Challenges Before Transformers}

\begin{itemize}
  \item RNNs processed sequences but suffered from vanishing gradients.
  \item LSTMs, with gating mechanisms, improved on RNNs but still struggled with long-range
        dependencies.
  \item Both required sequential processing, which slowed down training.
\end{itemize}

Researchers needed a solution that preserved long context and allowed for efficient training.

\subsection{2017: ``Attention Is All You Need'' --- The Birth of the Transformer}

In 2017, a Google research team published the groundbreaking paper
``Attention Is All You Need''. The Transformer introduced:
\begin{itemize}
  \item \textbf{Self-Attention:} Every word in a sentence relates directly to every other word.
  \item \textbf{Parallel Processing:} Eliminating the need for sequential steps.
  \item \textbf{Scalability:} Efficient training on GPUs allowed for much larger models.
\end{itemize}

\subsubsection{Self-Attention}

Instead of looking only at the few preceding words, a Transformer considers all words at once,
determining the relevance between each pair via:
\[
\mytext{Attention}(Q,K,V) = \mytext{softmax}\!\Bigl(\frac{QK^T}{\sqrt{d_k}}\Bigr)V
\]
where \(Q\) (Query), \(K\) (Key), and \(V\) (Value) encode word relationships.

\subsection{Transformers in NLP (2018--2020)}

Following the Transformer paper, the architecture was quickly adopted:
\begin{itemize}
  \item \textbf{BERT (2018, Google):} Used Transformers for bidirectional contextual
        language understanding.
  \item \textbf{XLNet (2019, Google/CMU):} Enhanced BERT with permutation-based training.
  \item \textbf{T5 (2019, Google):} Treated all NLP tasks as text-to-text transformations.
  \item \textbf{RoBERTa (2019, Facebook AI):} An optimized BERT variant with improved pretraining.
\end{itemize}

\subsection{Bridging to GPT}

While models like BERT were designed as encoders for understanding,
OpenAI took a different path:
\begin{itemize}
  \item \textbf{2018:} GPT-1 --- The first generative Transformer model.
  \item \textbf{2019:} GPT-2 --- Improved autoregressive text generation.
  \item \textbf{2020:} GPT-3 --- Revolutionized generative AI with human-like text output.
\end{itemize}

%------------------------------------------------------------------------------
% The GPT Revolution from 2020: Artificial Intelligence at the Next Level
%------------------------------------------------------------------------------

\section{The GPT Revolution from 2020: Artificial Intelligence at the Next Level}

\subsection{A Paradigm Shift}

From 2020 onward, large generative language models based on Transformers triggered an AI
revolution. While the 2010s saw improved language understanding via models like BERT,
OpenAI’s GPT-3 (2020) introduced a new quality in text generation.

Key differences include:
\begin{itemize}
  \item GPT models are autoregressive, meaning they generate text as well as understand it.
  \item They are based on the decoder part of the Transformer, unlike encoder-based models.
\end{itemize}

This breakthrough spurred new applications in chatbots, automated content creation,
AI-assisted programming, and more.

\subsection{GPT-3: The Breakthrough in Generative AI (2020)}

Released in June 2020, GPT-3:
\begin{itemize}
  \item Boasted 175 billion parameters, an unprecedented scale.
  \item Performed a wide variety of tasks from translation to code generation.
  \item Produced text so human-like that it was often indistinguishable from human writing.
\end{itemize}
Its success was driven by “few-shot learning,” where the model needed only a few examples
to perform a task.

\subsection{ChatGPT: Making AI Interactive (2022)}

In December 2022, OpenAI launched ChatGPT, enabling dialogue with large language models.
Key features included:
\begin{itemize}
  \item Reinforcement Learning from Human Feedback (RLHF) to improve safety and utility.
  \item The ability to remember conversational context.
  \item Wide accessibility to the public.
\end{itemize}
Within five days, ChatGPT reached one million users and sparked debates on automation,
ethics, and job displacement.

\subsection{GPT-4: Multimodality and Enhanced Intelligence (2023)}

In March 2023, GPT-4 was released with significant improvements:
\begin{itemize}
  \item \textbf{Multimodality:} The ability to understand and describe images.
  \item \textbf{Larger Context Windows:} Enhanced capacity to analyze longer texts.
  \item \textbf{Improved Accuracy:} Fewer factual errors through refined tuning.
\end{itemize}
Its applications range from coding and medical research to educational tools.

\subsection{Open-Source and New Competitors (2023--2024)}

Alongside proprietary models, the open-source AI community grew:
\begin{itemize}
  \item \textbf{Meta’s LLaMA (2023):} A powerful model accessible for research.
  \item \textbf{Mistral AI (2023):} Smaller but highly efficient models.
  \item \textbf{DeepSeek AI (2024):} New competitors from China offering alternative systems.
\end{itemize}
Companies such as Google (Gemini), Microsoft (Copilot), and Anthropic (Claude) also expanded
their generative AI offerings.

\subsection{The Next Step: Agents and Multimodal AI (2025)}

Development is accelerating:
\begin{itemize}
  \item \textbf{Autonomous AI Agents:} Systems that plan and execute complex tasks
        independently.
  \item \textbf{Enhanced Multimodality:} Integration of text, images, video, and audio in real time.
  \item \textbf{Personal AI Assistants:} Digital assistants capable of long-term, interactive
        engagement.
\end{itemize}

The GPT revolution has transformed our world, and the coming decade promises even greater
advancements.

%------------------------------------------------------------------------------
% AI Agents: Autonomous Systems of the Future
%------------------------------------------------------------------------------

\section{AI Agents: Autonomous Systems of the Future}

\subsection{From Language Models to Autonomous Agents}

While large language models like GPT-3 and GPT-4 generate impressive text, the next step is
to develop autonomous AI agents. These agents go beyond simple Q\&A systems and can plan,
execute, and optimize complex tasks independently.

\subsection{What is an AI Agent?}

An AI agent is a system that:
\begin{itemize}
  \item Pursues specific goals (e.g., research, coding, process optimization).
  \item Makes independent decisions based on available information.
  \item Interacts with external systems via tools and APIs.
\end{itemize}
They typically include:
\begin{itemize}
  \item A large language model as the “brain” for processing and decision-making.
  \item Tools and APIs to access external systems.
  \item Long-term memory for recalling past tasks or user preferences.
  \item Planning mechanisms to think several steps ahead.
\end{itemize}

\subsection{Existing AI Agents Today}

Examples include:
\begin{itemize}
  \item \textbf{Autonomous Research and Writing Agents:}
    \begin{itemize}
      \item \textbf{Auto-GPT (2023):} The first agent capable of setting its own goals and working
            autonomously.
      \item \textbf{BabyAGI (2023):} An agent that creates and refines long-term plans.
      \item \textbf{AgentGPT (2023):} A web-based agent that can independently undertake tasks.
    \end{itemize}
  \item \textbf{AI Agents for Software Development:}
    \begin{itemize}
      \item \textbf{GitHub Copilot X (2023--2024):} An agent integrated into IDEs for code generation.
      \item \textbf{Devin (2024, Cognition AI):} An agent that autonomously programs and debugs.
    \end{itemize}
  \item \textbf{Agents for Research and Science:}
    \begin{itemize}
      \item \textbf{Google DeepMind AlphaFold (2021--2023):} An agent that predicts protein
            structures and advances biological research.
      \item \textbf{Elicit AI (2023):} An agent that analyzes and summarizes scientific literature.
    \end{itemize}
  \item \textbf{Autonomous Business and Automation Agents:}
    \begin{itemize}
      \item \textbf{Microsoft Copilot for Office (2023--2024):} AI-powered automation for business
            processes.
      \item \textbf{Adept ACT-1 (2023--2024):} A multimodal agent that interacts with user interfaces.
    \end{itemize}
\end{itemize}

\subsection{The Future of AI Agents}

Looking ahead, we can expect:
\begin{itemize}
  \item \textbf{Multimodal AI Agents:} Systems processing text, images, audio, and video simultaneously.
  \item \textbf{Agents with True Long-Term Memory:} Models that remember contexts and previous
        interactions over long periods.
  \item \textbf{Physical AI Agents:} Integration of language models with robotics to perform complex,
        real-world tasks.
  \item \textbf{Autonomous Economic Systems:} AI agents managing supply chains, marketing strategies,
        or trading.
  \item \textbf{Fully AI-Driven Developer Teams:} Software agents handling complete projects from
        planning to implementation.
\end{itemize}

The coming years will reveal how much autonomy AI agents can truly assume.

\subsection{Publicly Available Literature on AI Agents}

For more on AI agents, consider these resources:
\begin{enumerate}
  \item Auto-GPT: An introduction to autonomous AI agents
        (\url{https://github.com/Torantulino/Auto-GPT}).
  \item BabyAGI: A simple implementation of an AI agent with long-term planning
        (\url{https://github.com/yoheinakajima/babyagi}).
  \item Devin: The first AI agent for software development
        (\url{https://www.cognition-labs.com/devin}).
  \item AlphaFold: Google’s AI agent for protein research
        (\url{https://www.nature.com/articles/s41586-021-03819-2}).
  \item Elicit AI: Autonomous agents for scientific research
        (\url{https://elicit.org/}).
  \item Adept ACT-1: Multimodal interaction agents
        (\url{https://www.adept.ai/blog/act-1}).
  \item GitHub Copilot X: The future of AI-assisted software development
        (\url{https://github.com/features/copilot-x}).
\end{enumerate}

% Retrieve Microsoft OneNote: https://aka.ms/GetOneNoteMobile
