{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3a08ce-6986-4185-bed9-8ed81bb823d6",
   "metadata": {},
   "source": [
    "# Query Ollama based on REQUESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cb0730-8039-4a01-929e-f92faf15772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# Example usage:\n",
    "print(query_ollama(\"What is the capital of Germany?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423487fd-8c9d-41f3-b317-48e32ab54e06",
   "metadata": {},
   "source": [
    "# Streaming response with Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c897a3a2-4f0f-427c-9253-521195b3197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Self**-attention! A fundamental building block of the **Transformer** architecture.\n",
       "\n",
       "\n",
       "\n",
       "**In** natural language processing (**NLP**), self-attention refers to the ability of a model to weigh and combine different parts of an input sequence with respect to each other. **In** other words, it allows the model to attend to different regions of the input sequence simultaneously, and then aggregate this information to produce a representation that captures the relationships between these regions.\n",
       "\n",
       "\n",
       "\n",
       "**In** the **Transformer** architecture, self-attention is used in three main components:\n",
       "\n",
       "\n",
       "\n",
       "1. ****Encoder****: **The** encoder takes in a sequence of tokens (e.g., words or characters) as input and outputs a continuous representation for each token.\n",
       "\n",
       "2. ****Decoder****: **The** decoder generates an output sequence one token at a time, using the encoder's output as context.\n",
       "\n",
       "3. ****Multi**-head attention**: **This** is a mechanism that allows the model to jointly attend to information from different representation dimensions (e.g., different word embeddings) at the same time.\n",
       "\n",
       "\n",
       "\n",
       "**Here**'s how self-attention works in **Transformers**:\n",
       "\n",
       "\n",
       "\n",
       "****Attention** mechanism**\n",
       "\n",
       "\n",
       "\n",
       "**Given** an input sequence `x = [x_1, ..., x_n]` of length `n`, where each `x_i` is a token embedding, the attention mechanism computes a weighted sum of these tokens. **The** goal is to produce a representation that captures the relationships between different parts of the input sequence.\n",
       "\n",
       "\n",
       "\n",
       "**The** attention process involves three main steps:\n",
       "\n",
       "\n",
       "\n",
       "1. ****Query** (Q)**: **Compute** a query vector `q_i` for each token `x_i`. **This** is typically done using a linear transformation applied to the token embedding.\n",
       "\n",
       "2. ****Key** (K)** and ****Value** (V)**: **Compute** key and value vectors `k_i` and `v_i`, respectively, for each token `x_i`. **These** are also typically computed using linear transformations applied to the token embedding.\n",
       "\n",
       "3. ****Attention** scores**: **Calculate** attention scores `scores[i] = softmax(Q * K^T / sqrt(d))`, where `d` is a hyperparameter (e.g., the dimensionality of the query and key vectors). **The** scores represent the relative importance of each token with respect to all other tokens.\n",
       "\n",
       "\n",
       "\n",
       "****Self**-attention**\n",
       "\n",
       "\n",
       "\n",
       "**In** self-attention, the attention mechanism is applied recursively to the output of the previous iteration. **This** allows the model to attend to different parts of the input sequence simultaneously and then aggregate this information.\n",
       "\n",
       "\n",
       "\n",
       "**The** self-attention process involves three main components:\n",
       "\n",
       "\n",
       "\n",
       "1. ****Query** (Q)**: **Compute** a query vector `q_i` for each token `x_i`, using the output of the previous attention layer as context.\n",
       "\n",
       "2. ****Key** (K)** and ****Value** (V)**: **Compute** key and value vectors `k_i` and `v_i`, respectively, for each token `x_i`, also using the output of the previous attention layer as context.\n",
       "\n",
       "3. ****Attention** scores**: **Calculate** attention scores `scores[i] = softmax(Q * K^T / sqrt(d))`. **The** scores represent the relative importance of each token with respect to all other tokens in the sequence.\n",
       "\n",
       "\n",
       "\n",
       "****Multi**-head attention**\n",
       "\n",
       "\n",
       "\n",
       "**To** capture different types of relationships between tokens, such as word order and semantic meaning, multi-head attention is used. **This** involves applying self-attention multiple times in parallel, using different sets of learnable weights. **The** output of these multiple attention heads is concatenated and linearly transformed to produce the final output.\n",
       "\n",
       "\n",
       "\n",
       "**In** summary, self-attention in **Transformers** allows the model to attend to different parts of an input sequence simultaneously, weighing their importance with respect to each other. **This** enables the model to capture complex relationships between tokens and generate coherent, contextualized representations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def format_markdown(text):\n",
    "    text = text.replace(\"\\n\", \"\\n\\n\")\n",
    "    text = re.sub(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', r'**\\1**', text)\n",
    "    text = re.sub(r'\\b([A-Z]{3,})\\b', r'**\\1**', text)\n",
    "    return text\n",
    "\n",
    "def stream_ollama_markdown(prompt, model=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\"model\": model, \"prompt\": prompt, \"stream\": True}\n",
    "\n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "    buffer = \"\"\n",
    "    output_display = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:\n",
    "            try:\n",
    "                data = json.loads(chunk.decode(\"utf-8\"))\n",
    "                text = data.get(\"response\", \"\")\n",
    "                if text:\n",
    "                    buffer += text\n",
    "                    clear_output(wait=True)\n",
    "                    output_display.update(Markdown(format_markdown(buffer)))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "    # Final display (in case last chunk isn’t shown)\n",
    "    clear_output(wait=True)\n",
    "    output_display.update(Markdown(format_markdown(buffer)))\n",
    "    # No return\n",
    "\n",
    "\n",
    "# Try it\n",
    "stream_ollama_markdown(\"Explain the concept of self-attention in Transformers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc3734-ec91-4e83-b89e-9627125b675f",
   "metadata": {},
   "source": [
    "# Preparing python based ollama access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0fdf83-22a8-4441-ae1c-78518f44d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from ollama) (2.11.2)\n",
      "Requirement already satisfied: anyio in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/rolan/all/ropy_wsl/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552764b-10e8-463d-bb28-8528cf3f9e9a",
   "metadata": {},
   "source": [
    "## First example, querying ollama, no streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585ff4cc-1c01-4618-8f42-f8d561c9dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3', messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f17cb3-be8d-4063-994d-83d27ab47ce7",
   "metadata": {},
   "source": [
    "## Token Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05baa68c-936f-441d-ba02-00853a4bce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Testing model: mistral\n",
      "⏳ Testing model: llama3\n",
      "⏳ Testing model: deepseek-r1:7b\n",
      "⏳ Testing model: deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"deepseek-r1:1.5b\"\n",
    "]\n",
    "\n",
    "prompt = \"Explain self-attention in two paragraphs.\"\n",
    "page_token_count = 333  # ≈ one page\n",
    "\n",
    "results = []\n",
    "\n",
    "def query_ollama(model, prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "    response = requests.post(url, json=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"⏳ Testing model: {model}\")\n",
    "    start = time.time()\n",
    "    response = query_ollama(model, prompt)\n",
    "    end = time.time()\n",
    "\n",
    "    tokens = len(response.split())\n",
    "    duration = end - start\n",
    "    tokens_per_sec = tokens / duration\n",
    "    seconds_per_page = page_token_count / tokens_per_sec\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model,\n",
    "        \"tokens\": tokens,\n",
    "        \"duration_sec\": duration,\n",
    "        \"tokens/sec\": tokens_per_sec,\n",
    "        \"sec/page\": seconds_per_page\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7d328a-270c-4dc4-bdcb-f93db1d2ab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokens</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>tokens/sec</th>\n",
       "      <th>sec/page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral</td>\n",
       "      <td>220</td>\n",
       "      <td>52.64</td>\n",
       "      <td>4.18</td>\n",
       "      <td>79.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3</td>\n",
       "      <td>270</td>\n",
       "      <td>49.18</td>\n",
       "      <td>5.49</td>\n",
       "      <td>60.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-r1:7b</td>\n",
       "      <td>613</td>\n",
       "      <td>121.55</td>\n",
       "      <td>5.04</td>\n",
       "      <td>66.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-r1:1.5b</td>\n",
       "      <td>561</td>\n",
       "      <td>28.68</td>\n",
       "      <td>19.56</td>\n",
       "      <td>17.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  tokens  duration_sec  tokens/sec  sec/page\n",
       "0           mistral     220         52.64        4.18     79.68\n",
       "1            llama3     270         49.18        5.49     60.66\n",
       "2    deepseek-r1:7b     613        121.55        5.04     66.03\n",
       "3  deepseek-r1:1.5b     561         28.68       19.56     17.02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display results nicely\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df = df.round(2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f856322-9584-4a07-b3db-5bce06ecc479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
