{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a869a4ae-24d2-485d-84f3-3629865a9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the Ollama model library\n",
    "OLLAMA_MODEL_LIBRARY_URL = \"https://ollama.com/library\"\n",
    "\n",
    "def fetch_available_models():\n",
    "    \"\"\"Scrapes the Ollama model library and correctly pairs models with their parameter sizes.\"\"\"\n",
    "    response = requests.get(OLLAMA_MODEL_LIBRARY_URL)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Failed to fetch model list from Ollama.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    models = []\n",
    "\n",
    "    # Find all links containing model names (/library/{model_name})\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if link[\"href\"].startswith(\"/library/\"):\n",
    "            model_name = link[\"href\"].split(\"/\")[-1]  # Extract model name from URL\n",
    "\n",
    "            # Look for size in the same block (search text within the same section)\n",
    "            parent = link.find_parent()\n",
    "            size_text = \"Unknown\"\n",
    "            if parent:\n",
    "                text_in_block = parent.get_text(\" \", strip=True)\n",
    "                size_matches = re.findall(r\"(\\d+(\\.\\d+)?b)\", text_in_block, re.IGNORECASE)\n",
    "                sizes = [s[0] for s in size_matches]  # Extract sizes like \"7b\", \"70b\", etc.\n",
    "\n",
    "                # Store the largest size or all available ones\n",
    "                size_text = \", \".join(sizes) if sizes else \"Unknown\"\n",
    "\n",
    "            models.append((model_name, size_text))\n",
    "\n",
    "    return models\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc2a961-7ee4-47ad-802c-2e258008a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fetching available models from the Ollama model library...\n",
      "\n",
      "üìå Available Ollama Models:\n",
      "1) gemma3 - 1b, 4b, 12b, 27b, 2) qwq - 32b, 3) deepseek-r1 - 1.5b, 7b, 8b, 14b, 32b, 70b, 671b, 4) llama3.3 - 70B, 70B, 405B, 70b, 5) phi4 - 14B, 14b, 6) llama3.2 - 1B, 3B, 1b, 3b, 7) llama3.1 - 8B, 70B, 405B, 8b, 70b, 405b, 8) nomic-embed-text - Unknown, 9) mistral - 7B, 7b, 10) llama3 - 8b, 70b, 11) qwen2.5 - 0.5b, 1.5b, 3b, 7b, 14b, 32b, 72b, 12) qwen2.5-coder - 0.5b, 1.5b, 3b, 7b, 14b, 32b, 13) llava - 7b, 13b, 34b, 14) qwen - 0.5B, 110B, 0.5b, 1.8b, 4b, 7b, 14b, 32b, 72b, 110b, 15) gemma - 2b, 7b, 16) qwen2 - 0.5b, 1.5b, 7b, 72b, 17) gemma2 - 2B, 9B, 27B, 2b, 9b, 27b, 18) llama2 - 7B, 70B, 7b, 13b, 70b, 19) phi3 - 3B, 14B, 3.8b, 14b, 20) mxbai-embed-large - Unknown, 21) codellama - 7b, 13b, 34b, 70b, 22) llama3.2-vision - 11B, 90B, 11b, 90b, 23) mistral-nemo - 12B, 12b, 24) tinyllama - 1.1B, 1.1b, 25) deepseek-v3 - 671B, 37B, 671b, 26) starcoder2 - 3B, 7B, 15B, 3b, 7b, 15b, 27) llama2-uncensored - 7b, 70b, 28) minicpm-v - 8b, 29) bge-m3 - Unknown, 30) deepseek-coder-v2 - 16b, 236b, 31) snowflake-arctic-embed - Unknown, 32) dolphin3 - 8B, 8b, 33) deepseek-coder - 1.3b, 6.7b, 33b, 34) mixtral - 7b, 22b, 7b, 22b, 35) olmo2 - 7B, 13B, 7b, 13b, 36) llava-llama3 - 8b, 37) codegemma - 2b, 7b, 38) dolphin-mixtral - 7b, 22b, 7b, 22b, 39) openthinker - 7b, 32b, 40) phi - 2.7B, 2.7b, 41) smollm2 - 1.7B, 1.7b, 42) mistral-small - 70B, 22b, 24b, 43) wizardlm2 - 7b, 22b, 44) all-minilm - Unknown, 45) dolphin-mistral - 7b, 46) orca-mini - 3b, 7b, 13b, 70b, 47) dolphin-llama3 - 8B, 70B, 8b, 70b, 48) command-r - 35b, 49) yi - 6b, 9b, 34b, 50) hermes3 - 3b, 8b, 70b, 405b, 51) phi3.5 - 3.8b, 52) zephyr - 7b, 141b, 53) codestral - 22b, 54) smollm - 1.7B, 1.7b, 55) granite-code - 3b, 8b, 20b, 34b, 56) wizard-vicuna-uncensored - 7B, 13B, 30B, 7b, 13b, 30b, 57) starcoder - 1b, 3b, 7b, 15b, 58) vicuna - 7b, 13b, 33b, 59) mistral-openorca - 7B, 7b, 60) moondream - 1.8b, 61) llama2-chinese - 7b, 13b, 62) openchat - 7b, 63) codegeex4 - 9b, 64) aya - 8b, 35b, 65) codeqwen - 7b, 66) openhermes - 7B, 67) deepseek-llm - 7b, 67b, 68) deepseek-v2 - 16b, 236b, 69) mistral-large - 123b, 70) glm4 - 9b, 71) stable-code - 3B, 7B, 3b, 72) tinydolphin - 1.1B, 1.1b, 73) nous-hermes2 - 10.7b, 34b, 74) qwen2-math - 1.5b, 7b, 72b, 75) command-r-plus - 104b, 76) wizardcoder - 33b, 77) bakllava - 7B, 7b, 78) stablelm2 - 1.6B, 12B, 1.6b, 12b, 79) neural-chat - 7b, 80) reflection - 70b, 81) wizard-math - 7b, 13b, 70b, 82) llama3-chatqa - 8b, 70b, 83) llama3-gradient - 8B, 8b, 70b, 84) sqlcoder - 7b, 15b, 85) bge-large - Unknown, 86) phi4-mini - 3.8b, 87) samantha-mistral - 7b, 88) granite3.1-dense - 2B, 8B, 2b, 8b, 89) dolphincoder - 7B, 15B, 7b, 15b, 90) xwinlm - 7b, 13b, 91) llava-phi3 - 3.8b, 92) nous-hermes - 7b, 13b, 93) phind-codellama - 34b, 94) starling-lm - 7b, 95) solar - 10.7B, 10.7b, 96) yarn-llama2 - 7b, 13b, 97) yi-coder - 1.5b, 9b, 98) athene-v2 - 72B, 72b, 99) internlm2 - 7B, 1.8b, 7b, 20b, 100) wizardlm - Unknown, 101) nemotron-mini - 4b, 102) deepscaler - 1.5B, 1.5B, 1.5b, 103) falcon - 7b, 40b, 180b, 104) granite3-dense - 2B, 8B, 2b, 8b, 105) nemotron - 70B, 70b, 106) dolphin-phi - 2.7B, 2.7b, 107) orca2 - 7b, 13b, 108) wizardlm-uncensored - 13b, 109) stable-beluga - 7b, 13b, 70b, 110) llama3-groq-tool-use - 8b, 70b, 111) granite3.2 - 2b, 8b, 112) paraphrase-multilingual - Unknown, 113) snowflake-arctic-embed2 - Unknown, 114) deepseek-v2.5 - 236b, 115) smallthinker - 3B, 3b, 116) aya-expanse - 8b, 32b, 117) meditron - 7b, 70b, 118) medllama2 - 7b, 119) granite3-moe - 1B, 3B, 1b, 3b, 120) falcon3 - 10B, 1b, 3b, 7b, 10b, 121) llama-pro - Unknown, 122) yarn-mistral - 7b, 123) nexusraven - 13B, 13b, 124) codeup - 13b, 125) everythinglm - 13b, 126) nous-hermes2-mixtral - 7b, 127) granite3.1-moe - 1B, 3B, 1b, 3b, 128) shieldgemma - 2b, 9b, 27b, 129) reader-lm - 0.5b, 1.5b, 130) granite3.2-vision - 2b, 131) marco-o1 - 7b, 132) exaone3.5 - 2.4B, 32B, 2.4b, 7.8b, 32b, 133) mathstral - 7B, 7b, 134) llama-guard3 - 1b, 8b, 135) solar-pro - 22b, 136) falcon2 - 11B, 11b, 137) stablelm-zephyr - 3b, 138) magicoder - 7B, 7b, 139) codebooga - 34b, 140) duckdb-nsql - 7B, 7b, 141) mistrallite - 7b, 142) wizard-vicuna - 13B, 13b, 143) command-r7b - 7b, 7b, 144) granite-embedding - Unknown, 145) opencoder - 1.5B, 8B, 1.5b, 8b, 146) nuextract - 3.8B, 3.8b, 147) megadolphin - 120b, 70b, 120b, 148) bespoke-minicheck - 7b, 149) notux - 7b, 150) open-orca-platypus2 - 13b, 151) notus - 7B, 7b, 152) exaone-deep - 2.4B, 32B, 2.4b, 7.8b, 32b, 153) goliath - 70B, 154) tulu3 - 8b, 70b, 155) r1-1776 - 70b, 671b, 156) firefunction-v2 - 70b, 157) dbrx - 132b, 158) granite3-guardian - 2B, 8B, 2b, 8b, 159) alfred - 40b, 160) sailor2 - 1B, 8B, 20B, 1b, 8b, 20b, 161) command-a - 111b, 162) command-r7b-arabic - 7b, 7B, 7b, "
     ]
    }
   ],
   "source": [
    "print(\"üîç Fetching available models from the Ollama model library...\")\n",
    "models = fetch_available_models()\n",
    "\n",
    "print(\"\\nüìå Available Ollama Models:\")\n",
    "for i, (name, size) in enumerate(models, 1):\n",
    "    print(f\"{i}) {name} - {size}\", end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55eaa9e-9b25-4aa7-982d-c626818e4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 80, Test dataset size: 20\n",
      "First training sample - X shape: torch.Size([10]), y shape: torch.Size([1])\n",
      "First training sample - X: [0.82811207 0.73099226 0.6460811  0.98673284 0.5317019  0.11775014\n",
      " 0.99185634 0.32278016 0.5298668  0.3955642 ], y: [0.8041298]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Generate synthetic data: 100 samples with 10 features each\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.rand(100, 1)\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and then split it into training and test sets\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Diagnostic Output\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Show shapes of the first batch\n",
    "first_train_sample = train_dataset[0]\n",
    "print(f\"First training sample - X shape: {first_train_sample[0].shape}, y shape: {first_train_sample[1].shape}\")\n",
    "\n",
    "# Show content of the first training sample\n",
    "print(f\"First training sample - X: {first_train_sample[0].numpy()}, y: {first_train_sample[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddce1a6b-0813-49ca-b34c-4dc4ded4bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 10\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5fe747-6462-4adc-af5b-ff16d9920aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "2) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "3) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "4) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "5) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Example: Iterate through one batch\n",
    "n = 1\n",
    "for batch_X, batch_y in dataloader:\n",
    "    print(f\"{n}) Batch X shape:\", batch_X.size())\n",
    "    print(\"      Batch y shape:\", batch_y.size())\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033eecfa-5c1a-4bac-be25-d16fe0fb46cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf78a62-4f58-45ce-9467-b4b2f66ef00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
